{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentDBVectorOperations import DocumentDBVectorSearch\n",
    "from Chunking import chunking, combine_text\n",
    "from DocumentLoader import load_and_read_pdf\n",
    "from GenerateEmbeddings import generate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure you are running ssh tunnel before executing this function\n"
     ]
    }
   ],
   "source": [
    "docdb_obj =  DocumentDBVectorSearch()\n",
    "docdb_obj.establish_connection()\n",
    "docdb_obj.create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = load_and_read_pdf(file_path = \"VisionTransformers.pdf\")\n",
    "\n",
    "final_text = combine_text(pages)\n",
    "\n",
    "chunks_of_text = chunking(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "docdb_obj.add_texts(chunks_of_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and a pure transformer applied directly to sequences of image patches can perform\n",
      "very well on image classiﬁcation tasks. When pre-trained on large amounts of\n",
      "data and transferred to multiple mid-sized or small image recognition benchmarks\n",
      "(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\n",
      "results compared to state-of-the-art convolutional networks while requiring sub-\n",
      "stantially fewer computational resources to train.1\n",
      "1 I NTRODUCTION\n",
      "Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\n",
      "the model of choice in natural language processing (NLP). The dominant approach is to pre-train on\n",
      "a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\n",
      "to Transformers’ computational efﬁciency and scalability, it has become possible to train models of\n",
      "unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\n",
      "=========================\n",
      "Transformers appear not to saturate within the range tried, motivating future scaling efforts.\n",
      "4.5 I NSPECTING VISION TRANSFORMER\n",
      "Input\n",
      " Attention\n",
      "Figure 6: Representative ex-\n",
      "amples of attention from the\n",
      "output token to the input\n",
      "space. See Appendix D.7 for\n",
      "details.\n",
      "To begin to understand how the Vision Transformer processes im-\n",
      "age data, we analyze its internal representations. The ﬁrst layer of\n",
      "the Vision Transformer linearly projects the ﬂattened patches into a\n",
      "lower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\n",
      "cipal components of the the learned embedding ﬁlters. The com-\n",
      "ponents resemble plausible basis functions for a low-dimensional\n",
      "representation of the ﬁne structure within each patch.\n",
      "After the projection, a learned position embedding is added to the\n",
      "patch representations. Figure 7 (center) shows that the model learns\n",
      "to encode distance within the image in the similarity of position em-\n",
      "beddings, i.e. closer patches tend to have more similar position em-\n",
      "=========================\n",
      "yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\n",
      "Thus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\n",
      "datasets, whilst being relatively cheap to pre-train.\n",
      "While these initial results are encouraging, many challenges remain. One is to apply ViT to other\n",
      "computer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\n",
      "et al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\n",
      "supervised pre-training methods. Our initial experiments show improvement from self-supervised\n",
      "pre-training, but there is still large gap between self-supervised and large-scale supervised pre-\n",
      "training. Finally, further scaling of ViT would likely lead to improved performance.\n",
      "ACKNOWLEDGEMENTS\n",
      "The work was performed in Berlin, Z ¨urich, and Amsterdam. We thank many colleagues at Google\n",
      "=========================\n",
      "1\n",
      "arXiv:2010.11929v2  [cs.CV]  3 Jun 2021Published as a conference paper at ICLR 2021\n",
      "inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\n",
      "when trained on insufﬁcient amounts of data.\n",
      "However, the picture changes if the models are trained on larger datasets (14M-300M images). We\n",
      "ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\n",
      "results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\n",
      "pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\n",
      "or beats state of the art on multiple image recognition benchmarks. In particular, the best model\n",
      "reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\n",
      "and 77.63% on the VTAB suite of 19 tasks.\n",
      "2 R ELATED WORK\n",
      "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is Vision Transformers?\"\n",
    "results = docdb_obj.vector_search(prompt, num_results=4)\n",
    "for cur_text in results:\n",
    "    print(cur_text['text'])\n",
    "    print(\"=========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure you are running ssh tunnel before executing this function\n"
     ]
    }
   ],
   "source": [
    "from DocumentDBVectorOperations import DocumentDBVectorSearch\n",
    "docdb_obj =  DocumentDBVectorSearch()\n",
    "docdb_obj.establish_connection()\n",
    "docdb_obj.create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RagPipeline import RAGPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_obj = RAGPipeline(groq_api_key=groq_api_key, Docdb_VS=docdb_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_obj.generate_answer(\"What is Vision Transformer? give answer in single paragraph\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Vision Transformer (ViT) is a type of transformer architecture that is applied directly to sequences of image patches to perform image classification tasks. It works by first linearly projecting the flattened patches into a lower-dimensional space, then adding a learned position embedding to the patch representations. The model learns to encode distance within the image in the similarity of position embeddings, allowing it to understand the structure of the image. When pre-trained on large amounts of data and transferred to multiple image recognition benchmarks, ViT attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train, making it a highly efficient and scalable architecture for computer vision tasks.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_results = docdb_obj.vector_search(\"What is vision transformers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and a pure transformer applied directly to sequences of image patches can perform\n",
      "very well on image classiﬁcation tasks. When pre-trained on large amounts of\n",
      "data and transferred to multiple mid-sized or small image recognition benchmarks\n",
      "(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\n",
      "results compared to state-of-the-art convolutional networks while requiring sub-\n",
      "stantially fewer computational resources to train.1\n",
      "1 I NTRODUCTION\n",
      "Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\n",
      "the model of choice in natural language processing (NLP). The dominant approach is to pre-train on\n",
      "a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\n",
      "to Transformers’ computational efﬁciency and scalability, it has become possible to train models of\n",
      "unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\n",
      "Transformers appear not to saturate within the range tried, motivating future scaling efforts.\n",
      "4.5 I NSPECTING VISION TRANSFORMER\n",
      "Input\n",
      " Attention\n",
      "Figure 6: Representative ex-\n",
      "amples of attention from the\n",
      "output token to the input\n",
      "space. See Appendix D.7 for\n",
      "details.\n",
      "To begin to understand how the Vision Transformer processes im-\n",
      "age data, we analyze its internal representations. The ﬁrst layer of\n",
      "the Vision Transformer linearly projects the ﬂattened patches into a\n",
      "lower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\n",
      "cipal components of the the learned embedding ﬁlters. The com-\n",
      "ponents resemble plausible basis functions for a low-dimensional\n",
      "representation of the ﬁne structure within each patch.\n",
      "After the projection, a learned position embedding is added to the\n",
      "patch representations. Figure 7 (center) shows that the model learns\n",
      "to encode distance within the image in the similarity of position em-\n",
      "beddings, i.e. closer patches tend to have more similar position em-\n",
      "yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\n",
      "Thus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\n",
      "datasets, whilst being relatively cheap to pre-train.\n",
      "While these initial results are encouraging, many challenges remain. One is to apply ViT to other\n",
      "computer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\n",
      "et al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\n",
      "supervised pre-training methods. Our initial experiments show improvement from self-supervised\n",
      "pre-training, but there is still large gap between self-supervised and large-scale supervised pre-\n",
      "training. Finally, further scaling of ViT would likely lead to improved performance.\n",
      "ACKNOWLEDGEMENTS\n",
      "The work was performed in Berlin, Z ¨urich, and Amsterdam. We thank many colleagues at Google\n"
     ]
    }
   ],
   "source": [
    "for result in vs_results:\n",
    "    print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperpal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
