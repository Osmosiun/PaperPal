{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentDBVectorOperations import DocumentDBVectorSearch\n",
    "from Chunking import chunking, combine_text\n",
    "from DocumentLoader import load_and_read_pdf\n",
    "from GenerateEmbeddings import generate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docdb_obj =  DocumentDBVectorSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = load_and_read_pdf(file_path = \"VisionTransformers.pdf\")\n",
    "\n",
    "final_text = combine_text(pages)\n",
    "\n",
    "chunks_of_text = chunking(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "docdb_obj.add_texts(chunks_of_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is Vision Transformer\"\n",
    "prompt_embed = generate_embeddings([query])[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_vss_index'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docdb_obj.collection.create_index ([(\"embedding\",\"vector\")], \n",
    "    vectorOptions= {\n",
    "        \"type\": \"hnsw\", \n",
    "        \"similarity\": \"cosine\",\n",
    "        \"dimensions\": 1024,\n",
    "        \"m\": 16,\n",
    "        \"efConstruction\": 64},\n",
    "    name=\"my_vss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection = {\n",
    "# \"_id\":0,\n",
    "# \"title\": 1, \n",
    "# \"overview\": 1}\n",
    "\n",
    "# #Semantic search function\n",
    "# def search_semantic(embedding):\n",
    "#     query = {\"vectorSearch\" : {\"vector\" : embedding, \"path\": \"embedding\", \"similarity\": \"cosine\", \"k\": 3}}\n",
    "#     results = docdb_obj.collection.aggregate([{'$search': query},{\"$project\": projection}])\n",
    "#     return list(results)\n",
    "\n",
    "# #Text search function\n",
    "# def search_text(keyword):\n",
    "#     results = collection.aggregate([{\"$match\": {\"$text\": {\"$search\": keyword}}},{\"$project\": projection},{\"$limit\": 3}])\n",
    "#     return list(results)\n",
    "\n",
    "# #Hybrid query function\n",
    "# def search_hybrid(keyword):\n",
    "#     results1 = search_semantic(keyword)[:2]\n",
    "#     results2 = search_text(keyword)[:2]\n",
    "#     combined_results = results1 + results2\n",
    "#     combined_results_as_tuples = [tuple(d.items()) for d in combined_results]\n",
    "#     union_result = list(set(combined_results_as_tuples))\n",
    "#     return union_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection = {\n",
    "# \"_id\":0,\n",
    "# \"title\": 1, \n",
    "# \"overview\": 1}\n",
    "\n",
    "#Semantic search function\n",
    "def search_semantic(embedding):\n",
    "    query = {\"vectorSearch\" : {\"vector\" : embedding, \"path\": \"embedding\", \"similarity\": \"cosine\", \"k\": 3}}\n",
    "    results = docdb_obj.collection.aggregate([{'$search': query}])\n",
    "    return list(results)\n",
    "\n",
    "#Text search function\n",
    "def search_text(keyword):\n",
    "    results = docdb_obj.collection.aggregate([{\"$match\": {\"$text\": {\"$search\": keyword}}},{\"$limit\": 3}])\n",
    "    return list(results)\n",
    "\n",
    "#Hybrid query function\n",
    "def search_hybrid(keyword):\n",
    "    results1 = search_semantic(keyword)[:2]\n",
    "    results2 = search_text(keyword)[:2]\n",
    "    combined_results = results1 + results2\n",
    "    combined_results_as_tuples = [tuple(d.items()) for d in combined_results]\n",
    "    union_result = list(set(combined_results_as_tuples))\n",
    "    return union_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_semantic(prompt_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = \"\"\n",
    "for result in results:\n",
    "\n",
    "    final_text += result['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers appear not to saturate within the range tried, motivating future scaling efforts.\\n4.5 I NSPECTING VISION TRANSFORMER\\nInput\\n Attention\\nFigure 6: Representative ex-\\namples of attention from the\\noutput token to the input\\nspace. See Appendix D.7 for\\ndetails.\\nTo begin to understand how the Vision Transformer processes im-\\nage data, we analyze its internal representations. The ﬁrst layer of\\nthe Vision Transformer linearly projects the ﬂattened patches into a\\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\\ncipal components of the the learned embedding ﬁlters. The com-\\nponents resemble plausible basis functions for a low-dimensional\\nrepresentation of the ﬁne structure within each patch.\\nAfter the projection, a learned position embedding is added to the\\npatch representations. Figure 7 (center) shows that the model learns\\nto encode distance within the image in the similarity of position em-\\nbeddings, i.e. closer patches tend to have more similar position em-and a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.1\\n1 I NTRODUCTION\\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With theyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\nThus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\\ndatasets, whilst being relatively cheap to pre-train.\\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\\nACKNOWLEDGEMENTS\\nThe work was performed in Berlin, Z ¨urich, and Amsterdam. We thank many colleagues at Google'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperpal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
