{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentDBVectorOperations import DocumentDBVectorSearch\n",
    "from Chunking import chunking, combine_text\n",
    "from DocumentLoader import load_and_read_pdf\n",
    "from GenerateEmbeddings import generate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure you are running ssh tunnel before executing this function\n"
     ]
    }
   ],
   "source": [
    "docdb_obj =  DocumentDBVectorSearch()\n",
    "docdb_obj.establish_connection()\n",
    "docdb_obj.create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete all documents successfully.\n"
     ]
    }
   ],
   "source": [
    "# docdb_obj.delete_all_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = load_and_read_pdf(file_path = \"VisionTransformers.pdf\")\n",
    "\n",
    "final_text = combine_text(pages)\n",
    "\n",
    "chunks_of_text = chunking(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "docdb_obj.add_texts(chunks_of_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\n",
      "et al., 2020b; He et al., 2020; Bachman et al., 2019; H´enaff et al., 2020) to future work. 5 C ONCLUSION\n",
      "We have explored the direct application of Transformers to image recognition. Unlike prior works\n",
      "using self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into\n",
      "the architecture apart from the initial patch extraction step. Instead, we interpret an image as a\n",
      "sequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\n",
      "yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets. Thus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\n",
      "datasets, whilst being relatively cheap to pre-train. While these initial results are encouraging, many challenges remain. One is to apply ViT to other\n",
      "computer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\n",
      "et al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\n",
      "supervised pre-training methods. Our initial experiments show improvement from self-supervised\n",
      "pre-training, but there is still large gap between self-supervised and large-scale supervised pre-\n",
      "training. Finally, further scaling of ViT would likely lead to improved performance. ACKNOWLEDGEMENTS\n",
      "The work was performed in Berlin, Z ¨urich, and Amsterdam. We thank many colleagues at Google\n",
      "for their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\n",
      "source release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\n",
      "=========================\n",
      "when trained on insufﬁcient amounts of data. However, the picture changes if the models are trained on larger datasets (14M-300M images). We\n",
      "ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\n",
      "results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\n",
      "pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\n",
      "or beats state of the art on multiple image recognition benchmarks. In particular, the best model\n",
      "reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\n",
      "and 77.63% on the VTAB suite of 19 tasks. 2 R ELATED WORK\n",
      "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\n",
      "come the state of the art method in many NLP tasks. Large Transformer-based models are often\n",
      "pre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\n",
      "uses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\n",
      "eling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other\n",
      "pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\n",
      "to apply Transformers in the context of image processing, several approximations have been tried in\n",
      "the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\n",
      "pixel instead of globally. Such local multi-head dot-product self attention blocks can completely\n",
      "replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\n",
      "line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\n",
      "=========================\n",
      "Published as a conference paper at ICLR 2021\n",
      "AN IMAGE IS WORTH 16X16 W ORDS :\n",
      "TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n",
      "Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\n",
      "Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\n",
      "Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n",
      "∗equal technical contribution, †equal advising\n",
      "Google Research, Brain Team\n",
      "{adosovitskiy, neilhoulsby}@google.com\n",
      "ABSTRACT\n",
      "While the Transformer architecture has become the de-facto standard for natural\n",
      "language processing tasks, its applications to computer vision remain limited.\n",
      "=========================\n",
      "2020). Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard\n",
      "Transformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\n",
      "into patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\n",
      "former. Image patches are treated the same way as tokens (words) in an NLP application. We train\n",
      "the model on image classiﬁcation in supervised fashion. When trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\n",
      "els yield modest accuracies of a few percentage points below ResNets of comparable size. This\n",
      "seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\n",
      "1Fine-tuning code and pre-trained models are available at https://github.com/\n",
      "google-research/vision_transformer\n",
      "1\n",
      "arXiv:2010.11929v2  [cs.CV]  3 Jun 2021Published as a conference paper at ICLR 2021\n",
      "inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\n",
      "when trained on insufﬁcient amounts of data. However, the picture changes if the models are trained on larger datasets (14M-300M images). We\n",
      "ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\n",
      "results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\n",
      "pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\n",
      "or beats state of the art on multiple image recognition benchmarks. In particular, the best model\n",
      "reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\n",
      "and 77.63% on the VTAB suite of 19 tasks. 2 R ELATED WORK\n",
      "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\n",
      "come the state of the art method in many NLP tasks. Large Transformer-based models are often\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is Vision Transformers?\"\n",
    "results = docdb_obj.vector_search(prompt, num_results=4)\n",
    "for cur_text in results:\n",
    "    print(cur_text['text'])\n",
    "    print(\"=========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and a pure transformer applied directly to sequences of image patches can perform\n",
      "very well on image classiﬁcation tasks. When pre-trained on large amounts of\n",
      "data and transferred to multiple mid-sized or small image recognition benchmarks\n",
      "(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\n",
      "results compared to state-of-the-art convolutional networks while requiring sub-\n",
      "stantially fewer computational resources to train.1\n",
      "1 I NTRODUCTION\n",
      "Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\n",
      "the model of choice in natural language processing (NLP). The dominant approach is to pre-train on\n",
      "a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\n",
      "to Transformers’ computational efﬁciency and scalability, it has become possible to train models of\n",
      "unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\n",
      "=========================\n",
      "Transformers appear not to saturate within the range tried, motivating future scaling efforts.\n",
      "4.5 I NSPECTING VISION TRANSFORMER\n",
      "Input\n",
      " Attention\n",
      "Figure 6: Representative ex-\n",
      "amples of attention from the\n",
      "output token to the input\n",
      "space. See Appendix D.7 for\n",
      "details.\n",
      "To begin to understand how the Vision Transformer processes im-\n",
      "age data, we analyze its internal representations. The ﬁrst layer of\n",
      "the Vision Transformer linearly projects the ﬂattened patches into a\n",
      "lower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\n",
      "cipal components of the the learned embedding ﬁlters. The com-\n",
      "ponents resemble plausible basis functions for a low-dimensional\n",
      "representation of the ﬁne structure within each patch.\n",
      "After the projection, a learned position embedding is added to the\n",
      "patch representations. Figure 7 (center) shows that the model learns\n",
      "to encode distance within the image in the similarity of position em-\n",
      "beddings, i.e. closer patches tend to have more similar position em-\n",
      "=========================\n",
      "yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\n",
      "Thus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\n",
      "datasets, whilst being relatively cheap to pre-train.\n",
      "While these initial results are encouraging, many challenges remain. One is to apply ViT to other\n",
      "computer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\n",
      "et al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\n",
      "supervised pre-training methods. Our initial experiments show improvement from self-supervised\n",
      "pre-training, but there is still large gap between self-supervised and large-scale supervised pre-\n",
      "training. Finally, further scaling of ViT would likely lead to improved performance.\n",
      "ACKNOWLEDGEMENTS\n",
      "The work was performed in Berlin, Z ¨urich, and Amsterdam. We thank many colleagues at Google\n",
      "=========================\n",
      "1\n",
      "arXiv:2010.11929v2  [cs.CV]  3 Jun 2021Published as a conference paper at ICLR 2021\n",
      "inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\n",
      "when trained on insufﬁcient amounts of data.\n",
      "However, the picture changes if the models are trained on larger datasets (14M-300M images). We\n",
      "ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\n",
      "results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\n",
      "pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\n",
      "or beats state of the art on multiple image recognition benchmarks. In particular, the best model\n",
      "reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\n",
      "and 77.63% on the VTAB suite of 19 tasks.\n",
      "2 R ELATED WORK\n",
      "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is Vision Transformers?\"\n",
    "results = docdb_obj.vector_search(prompt, num_results=4)\n",
    "for cur_text in results:\n",
    "    print(cur_text['text'])\n",
    "    print(\"=========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_id', 'text', 'embedding'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure you are running ssh tunnel before executing this function\n"
     ]
    }
   ],
   "source": [
    "from DocumentDBVectorOperations import DocumentDBVectorSearch\n",
    "docdb_obj =  DocumentDBVectorSearch()\n",
    "docdb_obj.establish_connection()\n",
    "docdb_obj.create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RagPipeline import RAGPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_obj = RAGPipeline(groq_api_key=groq_api_key, Docdb_VS=docdb_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_obj.generate_answer(\"What is Transformer?\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, a Transformer refers to a type of neural network architecture introduced by Vaswani et al. in 2017, initially designed for natural language processing (NLP) tasks, particularly machine translation. The Transformer architecture relies on self-attention mechanisms, which allow it to weigh the importance of different input elements relative to each other. This is different from traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), which have different strengths and are more commonly used in other domains like computer vision.\n",
      "\n",
      "In the context of the provided document, the Transformer architecture is applied to image recognition tasks, where it is referred to as the Vision Transformer (ViT). The ViT works by dividing an image into patches, flattening them, and then feeding these patches through a Transformer encoder. This approach allows the model to capture long-range dependencies and contextual information within the image, similar to how the original Transformer does for text sequences.\n",
      "\n",
      "The Transformer's ability to handle sequential data and its efficiency in parallelization have made it a highly successful and influential model in the field of NLP, leading to the development of large-scale pre-trained models like BERT and its successors. The application of Transformer architectures to vision tasks, as explored in the document, represents an exciting extension of these ideas into the domain of computer vision.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the documents, Vision Transformer (ViT) is a type of neural network architecture that applies the Transformer model, originally designed for Natural Language Processing (NLP) tasks, to image recognition tasks. It works by splitting an image into patches, providing the sequence of linear embeddings of these patches as input to a Transformer, and treating image patches the same way as tokens (words) in an NLP application. The Vision Transformer is trained on large datasets, such as ImageNet, and has achieved excellent results, approaching or beating state-of-the-art performance on multiple image recognition benchmarks.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperpal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
