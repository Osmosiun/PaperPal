{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from mlflow.models.signature import infer_signature\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import huggingface_hub\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "huggingface_hub.login(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "para1 = \"The Reddit dataset is a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or ‚Äúsubreddit‚Äù, that a post belongs to. 50 large communities have been sampled to build a post-to-post graph, connecting posts if the same user comments on both. In total this dataset contains 232,965 posts with an average degree of 492. The first 20 days are used for training and the remaining days for testing. For features, off-the-shelf 300-dimensional GloVe CommonCrawl word vectors are used.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Configure 4-bit quantization\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# if device == \"cuda\":\n",
    "#     model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         quantization_config=quantization_config,\n",
    "#         device_map=\"auto\"\n",
    "#     )\n",
    "# else:\n",
    "#     model_name = \"google/flan-t5-small\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "#     model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Summarize the paragraph: {input_paragraph} Answer should contain only summary and nothing else.\"\n",
    "template_input_variables = [\"input_paragraph\"]\n",
    "model_input = dict()\n",
    "model_input[\"input_paragraph\"] = para1\n",
    "prompt_template = PromptTemplate(\n",
    "            input_variables=template_input_variables,\n",
    "            template=template,\n",
    "        )\n",
    "formatted_prompt = prompt_template.format(**model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Reddit dataset contains 232,965 posts with an average degree of 492.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True)\n",
    "outputs = model.generate(**inputs, max_length=512, num_beams=2, early_stopping=True)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import huggingface_hub\n",
    "\n",
    "# MLflow tracking\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "\n",
    "# Login to Hugging Face\n",
    "huggingface_hub.login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model details\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom PythonModel wrapper\n",
    "class TextSummarizationModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model_name, device):\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, max_length = 500)\n",
    "        self.device = device\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "\n",
    "        input_paragraph = model_input[\"input_paragraph\"].iloc[0]\n",
    "\n",
    "        formatted_prompt = f\"Summarize the paragraph: {input_paragraph} Answer should contain only summary and nothing else.\"\n",
    "\n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True)\n",
    "        inputs = {key: value.to (self.device) for key, value in inputs.items()}\n",
    "\n",
    "        # Generate summary\n",
    "        outputs = self.model.generate(**inputs, max_length=512, num_beams=2, early_stopping=True)\n",
    "        summary = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        return summary\n",
    "\n",
    "# Create an instance of the custom model\n",
    "wrapped_model = TextSummarizationModel(model_name = \"google/flan-t5-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/633859552949112282', creation_time=1735791458842, experiment_id='633859552949112282', last_update_time=1735791458842, lifecycle_stage='active', name='Text Summarization Experiment', tags={}>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Text Summarization Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'SummarizationModel' already exists. Creating a new version of this model...\n",
      "2025/01/01 23:58:50 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: SummarizationModel, version 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run initial summarization experiment-4 at: http://127.0.0.1:5000/#/experiments/633859552949112282/runs/6cddf2d122104457b5fbbf583a7cdf25\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/633859552949112282\n",
      "Model logged successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '8' of model 'SummarizationModel'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"initial summarization experiment-4\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    mlflow.log_param(\"device\", device)\n",
    "\n",
    "    # Infer signature: Input is a DataFrame with a column \"input_paragraph\"\n",
    "    example_input = pd.DataFrame({\"input_paragraph\":[para1]})\n",
    "    example_output = wrapped_model.predict(None, example_input)\n",
    "    signature = infer_signature(example_input, example_output)\n",
    "\n",
    "    print(\"=\"*10)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"summarization_model\",\n",
    "        python_model=wrapped_model,\n",
    "        # signature=signature,\n",
    "        registered_model_name=\"SummarizationModel\",\n",
    "        input_example=example_input,\n",
    "    )\n",
    "\n",
    "print(\"Model logged successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:09<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "model_uri = \"models:/SummarizationModel/8\"  # Update with your model name/version\n",
    "model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = pd.DataFrame({\"input_paragraph\":[para1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_input = {\"input_paragraph\": para1}\n",
    "summary = model.predict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Reddit dataset contains 232,965 posts with an average degree of 492.'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperpal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
